---
id: case-studies-index
title: ATLAS Case Studies
sidebar_label: Overview
sidebar_position: 0
---

# ATLAS Case Studies

Case studies provide real-world examples of adversarial attacks against AI systems, including both actual incidents and security research exercises.

## All Case Studies (45)

- [[atlas/case-studies/evasion-of-deep-learning-detector-for-malware-c-c-traffic|AML.CS0000]] Evasion of Deep Learning Detector for Malware C&C Traffic — exercise — Palo Alto Networks malware detection system
- [[atlas/case-studies/botnet-domain-generation-algorithm-dga-detection-evasion|AML.CS0001]] Botnet Domain Generation Algorithm (DGA) Detection Evasion — exercise — Palo Alto Networks ML-based DGA detection module
- [[atlas/case-studies/virustotal-poisoning|AML.CS0002]] VirusTotal Poisoning — incident — VirusTotal
- [[atlas/case-studies/bypassing-cylance-s-ai-malware-detection|AML.CS0003]] Bypassing Cylance's AI Malware Detection — exercise — CylancePROTECT, Cylance Smart Antivirus
- [[atlas/case-studies/camera-hijack-attack-on-facial-recognition-system|AML.CS0004]] Camera Hijack Attack on Facial Recognition System — incident — Shanghai government tax office's facial recognition service
- [[atlas/case-studies/attack-on-machine-translation-services|AML.CS0005]] Attack on Machine Translation Services — exercise — Google Translate, Bing Translator, Systran Translate
- [[atlas/case-studies/clearviewai-misconfiguration|AML.CS0006]] ClearviewAI Misconfiguration — incident — Clearview AI facial recognition tool
- [[atlas/case-studies/gpt-2-model-replication|AML.CS0007]] GPT-2 Model Replication — exercise — OpenAI GPT-2
- [[atlas/case-studies/proofpoint-evasion|AML.CS0008]] ProofPoint Evasion — exercise — ProofPoint Email Protection System
- [[atlas/case-studies/tay-poisoning|AML.CS0009]] Tay Poisoning — incident — Microsoft's Tay AI Chatbot
- [[atlas/case-studies/microsoft-azure-service-disruption|AML.CS0010]] Microsoft Azure Service Disruption — exercise — Internal Microsoft Azure Service
- [[atlas/case-studies/microsoft-edge-ai-evasion|AML.CS0011]] Microsoft Edge AI Evasion — exercise — New Microsoft AI Product
- [[atlas/case-studies/face-identification-system-evasion-via-physical-countermeasures|AML.CS0012]] Face Identification System Evasion via Physical Countermeasures — exercise — Commercial Face Identification Service
- [[atlas/case-studies/backdoor-attack-on-deep-learning-models-in-mobile-apps|AML.CS0013]] Backdoor Attack on Deep Learning Models in Mobile Apps — exercise — ML-based Android Apps
- [[atlas/case-studies/confusing-antimalware-neural-networks|AML.CS0014]] Confusing Antimalware Neural Networks — exercise — Kaspersky's Antimalware ML Model
- [[atlas/case-studies/compromised-pytorch-dependency-chain|AML.CS0015]] Compromised PyTorch Dependency Chain — incident — PyTorch
- [[atlas/case-studies/achieving-code-execution-in-mathgpt-via-prompt-injection|AML.CS0016]] Achieving Code Execution in MathGPT via Prompt Injection — exercise — MathGPT (https://mathgpt.streamlit.app/)
- [[atlas/case-studies/bypassing-id-me-identity-verification|AML.CS0017]] Bypassing ID.me Identity Verification — incident — California Employment Development Department
- [[atlas/case-studies/arbitrary-code-execution-with-google-colab|AML.CS0018]] Arbitrary Code Execution with Google Colab — exercise — Google Colab
- [[atlas/case-studies/poisongpt|AML.CS0019]] PoisonGPT — exercise — HuggingFace Users
- [[atlas/case-studies/indirect-prompt-injection-threats-bing-chat-data-pirate|AML.CS0020]] Indirect Prompt Injection Threats: Bing Chat Data Pirate — exercise — Microsoft Bing Chat
- [[atlas/case-studies/chatgpt-conversation-exfiltration|AML.CS0021]] ChatGPT Conversation Exfiltration — exercise — OpenAI ChatGPT
- [[atlas/case-studies/chatgpt-package-hallucination|AML.CS0022]] ChatGPT Package Hallucination — exercise — ChatGPT users
- [[atlas/case-studies/shadowray|AML.CS0023]] ShadowRay — incident — Multiple systems
- [[atlas/case-studies/morris-ii-worm-rag-based-attack|AML.CS0024]] Morris II Worm: RAG-Based Attack — exercise — RAG-based e-mail assistant
- [[atlas/case-studies/web-scale-data-poisoning-split-view-attack|AML.CS0025]] Web-Scale Data Poisoning: Split-View Attack — exercise — 10 web-scale datasets
- [[atlas/case-studies/financial-transaction-hijacking-with-m365-copilot-as-an-insider|AML.CS0026]] Financial Transaction Hijacking with M365 Copilot as an Insider — exercise — Microsoft 365 Copilot
- [[atlas/case-studies/organization-confusion-on-hugging-face|AML.CS0027]] Organization Confusion on Hugging Face — exercise — Hugging Face users
- [[atlas/case-studies/ai-model-tampering-via-supply-chain-attack|AML.CS0028]] AI Model Tampering via Supply Chain Attack — exercise — Private Container Registries
- [[atlas/case-studies/google-bard-conversation-exfiltration|AML.CS0029]] Google Bard Conversation Exfiltration — exercise — Google Bard
- [[atlas/case-studies/llm-jacking|AML.CS0030]] LLM Jacking — incident — Cloud-Based LLM Services
- [[atlas/case-studies/malicious-models-on-hugging-face|AML.CS0031]] Malicious Models on Hugging Face — incident — Hugging Face users
- [[atlas/case-studies/attempted-evasion-of-ml-phishing-webpage-detection-system|AML.CS0032]] Attempted Evasion of ML Phishing Webpage Detection System — incident — Commercial ML Phishing Webpage Detector
- [[atlas/case-studies/live-deepfake-image-injection-to-evade-mobile-kyc-verification|AML.CS0033]] Live Deepfake Image Injection to Evade Mobile KYC Verification — exercise — Mobile facial authentication service
- [[atlas/case-studies/prokyc-deepfake-tool-for-account-fraud-attacks|AML.CS0034]] ProKYC: Deepfake Tool for Account Fraud Attacks — incident — KYC verification services
- [[atlas/case-studies/data-exfiltration-from-slack-ai-via-indirect-prompt-injection|AML.CS0035]] Data Exfiltration from Slack AI via Indirect Prompt Injection — exercise — Slack AI
- [[atlas/case-studies/aikatz-attacking-llm-desktop-applications|AML.CS0036]] AIKatz: Attacking LLM Desktop Applications — exercise — LLM Desktop Applications (Claude, ChatGPT, Copilot)
- [[atlas/case-studies/data-exfiltration-via-agent-tools-in-copilot-studio|AML.CS0037]] Data Exfiltration via Agent Tools in Copilot Studio — exercise — Copilot Studio Customer Service Agent
- [[atlas/case-studies/planting-instructions-for-delayed-automatic-ai-agent-tool-invocation|AML.CS0038]] Planting Instructions for Delayed Automatic AI Agent Tool Invocation — exercise — Google Gemini
- [[atlas/case-studies/living-off-ai-prompt-injection-via-jira-service-management|AML.CS0039]] Living Off AI: Prompt Injection via Jira Service Management — exercise — Atlassian MCP, Jira Service Management
- [[atlas/case-studies/hacking-chatgpt-s-memories-with-prompt-injection|AML.CS0040]] Hacking ChatGPT’s Memories with Prompt Injection — exercise — OpenAI ChatGPT
- [[atlas/case-studies/rules-file-backdoor-supply-chain-attack-on-ai-coding-assistants|AML.CS0041]] Rules File Backdoor: Supply Chain Attack on AI Coding Assistants — exercise — Cursor, GitHub Copilot
- [[atlas/case-studies/sesameop-novel-backdoor-uses-openai-assistants-api-for-command-and-control|AML.CS0042]] SesameOp: Novel backdoor uses OpenAI Assistants API for command and control — incident — OpenAI Assistants API
- [[atlas/case-studies/malware-prototype-with-embedded-prompt-injection|AML.CS0043]] Malware Prototype with Embedded Prompt Injection — incident — LLM malware detectors, LLM malware analysis and reverse engineering tools
- [[atlas/case-studies/lamehug-malware-leveraging-dynamic-ai-generated-commands|AML.CS0044]] LAMEHUG: Malware Leveraging Dynamic AI-Generated Commands — incident — Ukraine’s security and defense sector

## Case Study Types

- **Exercise**: Security research or red team exercises
- **Incident**: Real-world attacks or security incidents

## References

MITRE Corporation. *ATLAS Case Studies*. Available at: https://atlas.mitre.org/studies
