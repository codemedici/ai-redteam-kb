---
id: confusing-antimalware-neural-networks
title: "AML.CS0014: Confusing Antimalware Neural Networks"
sidebar_label: "Confusing Antimalware Neural Networks"
sidebar_position: 15
---

# AML.CS0014: Confusing Antimalware Neural Networks

## Summary

Cloud storage and computations have become popular platforms for deploying ML malware detectors.
In such cases, the features for models are built on users' systems and then sent to cybersecurity company servers.
The Kaspersky ML research team explored this gray-box scenario and showed that feature knowledge is enough for an adversarial attack on ML models.

They attacked one of Kaspersky's antimalware ML models without white-box access to it and successfully evaded detection for most of the adversarially modified malware files.

## Metadata

- **Case Study ID:** AML.CS0014
- **Incident Date:** 2021
- **Type:** exercise
- **Target:** Kaspersky's Antimalware ML Model
- **Actor:** Kaspersky ML Research Team

## Attack Procedure

The following steps outline the attack procedure:

### Step 1: Search Open AI Vulnerability Analysis

**Tactic:** [[reconnaissance|AML.TA0002: Reconnaissance]]
**Technique:** [[search-open-ai-vulnerability-analysis|AML.T0001: Search Open AI Vulnerability Analysis]]

The researchers performed a review of adversarial ML attacks on antimalware products.
They discovered that techniques borrowed from attacks on image classifiers have been successfully applied to the antimalware domain.
However, it was not clear if these approaches were effective against the ML component of production antimalware solutions.

### Step 2: Search Victim-Owned Websites

**Tactic:** [[reconnaissance|AML.TA0002: Reconnaissance]]
**Technique:** [[search-victim-owned-websites|AML.T0003: Search Victim-Owned Websites]]

Kaspersky's use of ML-based antimalware detectors is publicly documented on their website. In practice, an adversary could use this for targeting.

### Step 3: AI-Enabled Product or Service

**Tactic:** [[ai-model-access|AML.TA0000: AI Model Access]]
**Technique:** [[ai-enabled-product-or-service|AML.T0047: AI-Enabled Product or Service]]

The researchers used access to the target ML-based antimalware product throughout this case study.
This product scans files on the user's system, extracts features locally, then sends them to the cloud-based ML malware detector for classification.
Therefore, the researchers had only black-box access to the malware detector itself, but could learn valuable information for constructing the attack from the feature extractor.

### Step 4: Datasets

**Tactic:** [[resource-development|AML.TA0003: Resource Development]]
**Technique:** [[datasets|AML.T0002.000: Datasets]]

The researchers collected a dataset of malware and clean files.
They scanned the dataset with the target ML-based antimalware solution and labeled the samples according to the ML detector's predictions.

### Step 5: Create Proxy AI Model

**Tactic:** [[ai-attack-staging|AML.TA0001: AI Attack Staging]]
**Technique:** [[create-proxy-ai-model|AML.T0005: Create Proxy AI Model]]

A proxy model was trained on the labeled dataset of malware and clean files.
The researchers experimented with a variety of model architectures.

### Step 6: Adversarial AI Attacks

**Tactic:** [[resource-development|AML.TA0003: Resource Development]]
**Technique:** [[adversarial-ai-attacks|AML.T0017.000: Adversarial AI Attacks]]

By reverse engineering the local feature extractor, the researchers could collect information about the input features, used for the cloud-based ML detector.
The model collects PE Header features, section features and section data statistics, and file strings information.
A gradient based adversarial algorithm for executable files was developed.
The algorithm manipulates file features to avoid detection by the proxy model, while still containing the same malware payload

### Step 7: Black-Box Transfer

**Tactic:** [[ai-attack-staging|AML.TA0001: AI Attack Staging]]
**Technique:** [[black-box-transfer|AML.T0043.002: Black-Box Transfer]]

Using a developed gradient-driven algorithm, malicious adversarial files for the proxy model were constructed from the malware files for black-box transfer to the target model.

### Step 8: Verify Attack

**Tactic:** [[ai-attack-staging|AML.TA0001: AI Attack Staging]]
**Technique:** [[verify-attack|AML.T0042: Verify Attack]]

The adversarial malware files were tested against the target antimalware solution to verify their efficacy.

### Step 9: Evade AI Model

**Tactic:** [[defense-evasion|AML.TA0007: Defense Evasion]]
**Technique:** [[evade-ai-model|AML.T0015: Evade AI Model]]

The researchers demonstrated that for most of the adversarial files, the antimalware model was successfully evaded.
In practice, an adversary could deploy their adversarially crafted malware and infect systems while evading detection.


## Tactics and Techniques Used


| Step | Tactic | Technique |
|---|---|---|
| 1 | [[reconnaissance|AML.TA0002: Reconnaissance]] | [[search-open-ai-vulnerability-analysis|AML.T0001: Search Open AI Vulnerability Analysis]] |
| 2 | [[reconnaissance|AML.TA0002: Reconnaissance]] | [[search-victim-owned-websites|AML.T0003: Search Victim-Owned Websites]] |
| 3 | [[ai-model-access|AML.TA0000: AI Model Access]] | [[ai-enabled-product-or-service|AML.T0047: AI-Enabled Product or Service]] |
| 4 | [[resource-development|AML.TA0003: Resource Development]] | [[datasets|AML.T0002.000: Datasets]] |
| 5 | [[ai-attack-staging|AML.TA0001: AI Attack Staging]] | [[create-proxy-ai-model|AML.T0005: Create Proxy AI Model]] |
| 6 | [[resource-development|AML.TA0003: Resource Development]] | [[adversarial-ai-attacks|AML.T0017.000: Adversarial AI Attacks]] |
| 7 | [[ai-attack-staging|AML.TA0001: AI Attack Staging]] | [[black-box-transfer|AML.T0043.002: Black-Box Transfer]] |
| 8 | [[ai-attack-staging|AML.TA0001: AI Attack Staging]] | [[verify-attack|AML.T0042: Verify Attack]] |
| 9 | [[defense-evasion|AML.TA0007: Defense Evasion]] | [[evade-ai-model|AML.T0015: Evade AI Model]] |



## External References

- Article, "How to confuse antimalware neural networks. Adversarial attacks and protection" Available at: https://securelist.com/how-to-confuse-antimalware-neural-networks-adversarial-attacks-and-protection/102949/


## References

MITRE Corporation. *Confusing Antimalware Neural Networks (AML.CS0014)*. MITRE ATLAS. Available at: https://atlas.mitre.org/studies/AML.CS0014
