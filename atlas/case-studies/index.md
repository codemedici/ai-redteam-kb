---
id: case-studies-index
title: ATLAS Case Studies
sidebar_label: Overview
sidebar_position: 0
---

# ATLAS Case Studies

Case studies provide real-world examples of adversarial attacks against AI systems, including both actual incidents and security research exercises.

## All Case Studies (45)

| ID | Case Study | Type | Target |
|---|---|---|---|
| [[evasion-of-deep-learning-detector-for-malware-c-c-traffic|AML.CS0000]] | **Evasion of Deep Learning Detector for Malware C&C Traffic** | exercise | Palo Alto Networks malware detection system |
| [[botnet-domain-generation-algorithm-dga-detection-evasion|AML.CS0001]] | **Botnet Domain Generation Algorithm (DGA) Detection Evasion** | exercise | Palo Alto Networks ML-based DGA detection module |
| [[virustotal-poisoning|AML.CS0002]] | **VirusTotal Poisoning** | incident | VirusTotal |
| [[bypassing-cylance-s-ai-malware-detection|AML.CS0003]] | **Bypassing Cylance's AI Malware Detection** | exercise | CylancePROTECT, Cylance Smart Antivirus |
| [[camera-hijack-attack-on-facial-recognition-system|AML.CS0004]] | **Camera Hijack Attack on Facial Recognition System** | incident | Shanghai government tax office's facial recognition service |
| [[attack-on-machine-translation-services|AML.CS0005]] | **Attack on Machine Translation Services** | exercise | Google Translate, Bing Translator, Systran Translate |
| [[clearviewai-misconfiguration|AML.CS0006]] | **ClearviewAI Misconfiguration** | incident | Clearview AI facial recognition tool |
| [[gpt-2-model-replication|AML.CS0007]] | **GPT-2 Model Replication** | exercise | OpenAI GPT-2 |
| [[proofpoint-evasion|AML.CS0008]] | **ProofPoint Evasion** | exercise | ProofPoint Email Protection System |
| [[tay-poisoning|AML.CS0009]] | **Tay Poisoning** | incident | Microsoft's Tay AI Chatbot |
| [[microsoft-azure-service-disruption|AML.CS0010]] | **Microsoft Azure Service Disruption** | exercise | Internal Microsoft Azure Service |
| [[microsoft-edge-ai-evasion|AML.CS0011]] | **Microsoft Edge AI Evasion** | exercise | New Microsoft AI Product |
| [[face-identification-system-evasion-via-physical-countermeasures|AML.CS0012]] | **Face Identification System Evasion via Physical Countermeasures** | exercise | Commercial Face Identification Service |
| [[backdoor-attack-on-deep-learning-models-in-mobile-apps|AML.CS0013]] | **Backdoor Attack on Deep Learning Models in Mobile Apps** | exercise | ML-based Android Apps |
| [[confusing-antimalware-neural-networks|AML.CS0014]] | **Confusing Antimalware Neural Networks** | exercise | Kaspersky's Antimalware ML Model |
| [[compromised-pytorch-dependency-chain|AML.CS0015]] | **Compromised PyTorch Dependency Chain** | incident | PyTorch |
| [[achieving-code-execution-in-mathgpt-via-prompt-injection|AML.CS0016]] | **Achieving Code Execution in MathGPT via Prompt Injection** | exercise | MathGPT (https://mathgpt.streamlit.app/) |
| [[bypassing-id-me-identity-verification|AML.CS0017]] | **Bypassing ID.me Identity Verification** | incident | California Employment Development Department |
| [[arbitrary-code-execution-with-google-colab|AML.CS0018]] | **Arbitrary Code Execution with Google Colab** | exercise | Google Colab |
| [[poisongpt|AML.CS0019]] | **PoisonGPT** | exercise | HuggingFace Users |
| [[indirect-prompt-injection-threats-bing-chat-data-pirate|AML.CS0020]] | **Indirect Prompt Injection Threats: Bing Chat Data Pirate** | exercise | Microsoft Bing Chat |
| [[chatgpt-conversation-exfiltration|AML.CS0021]] | **ChatGPT Conversation Exfiltration** | exercise | OpenAI ChatGPT |
| [[chatgpt-package-hallucination|AML.CS0022]] | **ChatGPT Package Hallucination** | exercise | ChatGPT users |
| [[shadowray|AML.CS0023]] | **ShadowRay** | incident | Multiple systems |
| [[morris-ii-worm-rag-based-attack|AML.CS0024]] | **Morris II Worm: RAG-Based Attack** | exercise | RAG-based e-mail assistant |
| [[web-scale-data-poisoning-split-view-attack|AML.CS0025]] | **Web-Scale Data Poisoning: Split-View Attack** | exercise | 10 web-scale datasets |
| [[financial-transaction-hijacking-with-m365-copilot-as-an-insider|AML.CS0026]] | **Financial Transaction Hijacking with M365 Copilot as an Insider** | exercise | Microsoft 365 Copilot |
| [[organization-confusion-on-hugging-face|AML.CS0027]] | **Organization Confusion on Hugging Face** | exercise | Hugging Face users |
| [[ai-model-tampering-via-supply-chain-attack|AML.CS0028]] | **AI Model Tampering via Supply Chain Attack** | exercise | Private Container Registries |
| [[google-bard-conversation-exfiltration|AML.CS0029]] | **Google Bard Conversation Exfiltration** | exercise | Google Bard |
| [[llm-jacking|AML.CS0030]] | **LLM Jacking** | incident | Cloud-Based LLM Services |
| [[malicious-models-on-hugging-face|AML.CS0031]] | **Malicious Models on Hugging Face** | incident | Hugging Face users |
| [[attempted-evasion-of-ml-phishing-webpage-detection-system|AML.CS0032]] | **Attempted Evasion of ML Phishing Webpage Detection System** | incident | Commercial ML Phishing Webpage Detector |
| [[live-deepfake-image-injection-to-evade-mobile-kyc-verification|AML.CS0033]] | **Live Deepfake Image Injection to Evade Mobile KYC Verification** | exercise | Mobile facial authentication service |
| [[prokyc-deepfake-tool-for-account-fraud-attacks|AML.CS0034]] | **ProKYC: Deepfake Tool for Account Fraud Attacks** | incident | KYC verification services |
| [[data-exfiltration-from-slack-ai-via-indirect-prompt-injection|AML.CS0035]] | **Data Exfiltration from Slack AI via Indirect Prompt Injection** | exercise | Slack AI |
| [[aikatz-attacking-llm-desktop-applications|AML.CS0036]] | **AIKatz: Attacking LLM Desktop Applications** | exercise | LLM Desktop Applications (Claude, ChatGPT, Copilot) |
| [[data-exfiltration-via-agent-tools-in-copilot-studio|AML.CS0037]] | **Data Exfiltration via Agent Tools in Copilot Studio** | exercise | Copilot Studio Customer Service Agent |
| [[planting-instructions-for-delayed-automatic-ai-agent-tool-invocation|AML.CS0038]] | **Planting Instructions for Delayed Automatic AI Agent Tool Invocation** | exercise | Google Gemini |
| [[living-off-ai-prompt-injection-via-jira-service-management|AML.CS0039]] | **Living Off AI: Prompt Injection via Jira Service Management** | exercise | Atlassian MCP, Jira Service Management |
| [[hacking-chatgpt-s-memories-with-prompt-injection|AML.CS0040]] | **Hacking ChatGPT’s Memories with Prompt Injection** | exercise | OpenAI ChatGPT |
| [[rules-file-backdoor-supply-chain-attack-on-ai-coding-assistants|AML.CS0041]] | **Rules File Backdoor: Supply Chain Attack on AI Coding Assistants** | exercise | Cursor, GitHub Copilot |
| [[sesameop-novel-backdoor-uses-openai-assistants-api-for-command-and-control|AML.CS0042]] | **SesameOp: Novel backdoor uses OpenAI Assistants API for command and control** | incident | OpenAI Assistants API |
| [[malware-prototype-with-embedded-prompt-injection|AML.CS0043]] | **Malware Prototype with Embedded Prompt Injection** | incident | LLM malware detectors, LLM malware analysis and reverse engineering tools |
| [[lamehug-malware-leveraging-dynamic-ai-generated-commands|AML.CS0044]] | **LAMEHUG: Malware Leveraging Dynamic AI-Generated Commands** | incident | Ukraine’s security and defense sector |

## Case Study Types

- **Exercise**: Security research or red team exercises
- **Incident**: Real-world attacks or security incidents

## References

MITRE Corporation. *ATLAS Case Studies*. Available at: https://atlas.mitre.org/studies
