---
id: impact
title: "AML.TA0011: Impact"
sidebar_label: "Impact"
sidebar_position: 16
---

# AML.TA0011: Impact

The adversary is trying to manipulate, interrupt, erode confidence in, or destroy your AI systems and data.

Impact consists of techniques that adversaries use to disrupt availability or compromise integrity by manipulating business and operational processes.
Techniques used for impact can include destroying or tampering with data.
In some cases, business processes can look fine, but may have been altered to benefit the adversaries' goals.
These techniques might be used by adversaries to follow through on their end goal or to provide cover for a confidentiality breach.

## Metadata

- **Tactic ID:** AML.TA0011
- **Created:** January 24, 2022
- **Last Modified:** April 9, 2025
- **MITRE ATT&CK Reference:** [TA0040](https://attack.mitre.org/tactics/TA0040/)

## Techniques (8)

The following techniques can be used to achieve this tactic:


| Technique ID | Name | Maturity |
|---|---|---|
| [[atlas/techniques/initial-access/evade-ai-model|AML.T0015]] | Evade AI Model | realized |
| [[atlas/techniques/impact/denial-of-ai-service|AML.T0029]] | Denial of AI Service | demonstrated |
| [[atlas/techniques/impact/spamming-ai-system-with-chaff-data|AML.T0046]] | Spamming AI System with Chaff Data | feasible |
| [[atlas/techniques/impact/erode-ai-model-integrity|AML.T0031]] | Erode AI Model Integrity | realized |
| [[atlas/techniques/impact/cost-harvesting|AML.T0034]] | Cost Harvesting | feasible |
| [[atlas/techniques/impact/external-harms/external-harms-overview|AML.T0048]] | External Harms | realized |
| [[atlas/techniques/impact/erode-dataset-integrity|AML.T0059]] | Erode Dataset Integrity | demonstrated |
| [[atlas/techniques/impact/data-destruction-via-ai-agent-tool-invocation|AML.T0101]] | Data Destruction via AI Agent Tool Invocation | feasible |


## Case Studies (30)


The following case studies demonstrate this tactic:

- [[atlas/case-studies/camera-hijack-attack-on-facial-recognition-system|AML.CS0004: Camera Hijack Attack on Facial Recognition System]]
- [[atlas/case-studies/attack-on-machine-translation-services|AML.CS0005: Attack on Machine Translation Services]]
- [[atlas/case-studies/clearviewai-misconfiguration|AML.CS0006: ClearviewAI Misconfiguration]]
- [[atlas/case-studies/proofpoint-evasion|AML.CS0008: ProofPoint Evasion]]
- [[atlas/case-studies/tay-poisoning|AML.CS0009: Tay Poisoning]]
- [[atlas/case-studies/microsoft-azure-service-disruption|AML.CS0010: Microsoft Azure Service Disruption]]
- [[atlas/case-studies/microsoft-edge-ai-evasion|AML.CS0011: Microsoft Edge AI Evasion]]
- [[atlas/case-studies/face-identification-system-evasion-via-physical-countermeasures|AML.CS0012: Face Identification System Evasion via Physical Countermeasures]]
- [[atlas/case-studies/backdoor-attack-on-deep-learning-models-in-mobile-apps|AML.CS0013: Backdoor Attack on Deep Learning Models in Mobile Apps]]
- [[atlas/case-studies/achieving-code-execution-in-mathgpt-via-prompt-injection|AML.CS0016: Achieving Code Execution in MathGPT via Prompt Injection]]
- [[atlas/case-studies/bypassing-id-me-identity-verification|AML.CS0017: Bypassing ID.me Identity Verification]]
- [[atlas/case-studies/arbitrary-code-execution-with-google-colab|AML.CS0018: Arbitrary Code Execution with Google Colab]]
- [[atlas/case-studies/poisongpt|AML.CS0019: PoisonGPT]]
- [[atlas/case-studies/indirect-prompt-injection-threats-bing-chat-data-pirate|AML.CS0020: Indirect Prompt Injection Threats: Bing Chat Data Pirate]]
- [[atlas/case-studies/chatgpt-conversation-exfiltration|AML.CS0021: ChatGPT Conversation Exfiltration]]
- [[atlas/case-studies/chatgpt-package-hallucination|AML.CS0022: ChatGPT Package Hallucination]]
- [[atlas/case-studies/shadowray|AML.CS0023: ShadowRay]]
- [[atlas/case-studies/morris-ii-worm-rag-based-attack|AML.CS0024: Morris II Worm: RAG-Based Attack]]
- [[atlas/case-studies/web-scale-data-poisoning-split-view-attack|AML.CS0025: Web-Scale Data Poisoning: Split-View Attack]]
- [[atlas/case-studies/financial-transaction-hijacking-with-m365-copilot-as-an-insider|AML.CS0026: Financial Transaction Hijacking with M365 Copilot as an Insider]]
- [[atlas/case-studies/organization-confusion-on-hugging-face|AML.CS0027: Organization Confusion on Hugging Face]]
- [[atlas/case-studies/ai-model-tampering-via-supply-chain-attack|AML.CS0028: AI Model Tampering via Supply Chain Attack]]
- [[atlas/case-studies/google-bard-conversation-exfiltration|AML.CS0029: Google Bard Conversation Exfiltration]]
- [[atlas/case-studies/llm-jacking|AML.CS0030: LLM Jacking]]
- [[atlas/case-studies/attempted-evasion-of-ml-phishing-webpage-detection-system|AML.CS0032: Attempted Evasion of ML Phishing Webpage Detection System]]
- [[atlas/case-studies/live-deepfake-image-injection-to-evade-mobile-kyc-verification|AML.CS0033: Live Deepfake Image Injection to Evade Mobile KYC Verification]]
- [[atlas/case-studies/prokyc-deepfake-tool-for-account-fraud-attacks|AML.CS0034: ProKYC: Deepfake Tool for Account Fraud Attacks]]
- [[atlas/case-studies/aikatz-attacking-llm-desktop-applications|AML.CS0036: AIKatz: Attacking LLM Desktop Applications]]
- [[atlas/case-studies/hacking-chatgpt-s-memories-with-prompt-injection|AML.CS0040: Hacking ChatGPTâ€™s Memories with Prompt Injection]]
- [[atlas/case-studies/rules-file-backdoor-supply-chain-attack-on-ai-coding-assistants|AML.CS0041: Rules File Backdoor: Supply Chain Attack on AI Coding Assistants]]


## References

MITRE Corporation. *Impact (AML.TA0011)*. MITRE ATLAS. Available at: https://atlas.mitre.org/tactics/AML.TA0011
