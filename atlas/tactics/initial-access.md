---
id: initial-access
title: "AML.TA0004: Initial Access"
sidebar_label: "Initial Access"
sidebar_position: 3
---

# AML.TA0004: Initial Access

The adversary is trying to gain access to the AI system.

The target system could be a network, mobile device, or an edge device such as a sensor platform.
The AI capabilities used by the system could be local with onboard or cloud-enabled AI capabilities.

Initial Access consists of techniques that use various entry vectors to gain their initial foothold within the system.

## Metadata

- **Tactic ID:** AML.TA0004
- **Created:** January 24, 2022
- **Last Modified:** April 9, 2025
- **MITRE ATT&CK Reference:** [TA0001](https://attack.mitre.org/tactics/TA0001/)

## Techniques (7)

The following techniques can be used to achieve this tactic:


| Technique ID | Name | Maturity |
|---|---|---|
| [[ai-supply-chain-compromise|AML.T0010]] | AI Supply Chain Compromise | realized |
| [[valid-accounts|AML.T0012]] | Valid Accounts | realized |
| [[evade-ai-model|AML.T0015]] | Evade AI Model | realized |
| [[exploit-public-facing-application|AML.T0049]] | Exploit Public-Facing Application | realized |
| [[phishing|AML.T0052]] | Phishing | realized |
| [[drive-by-compromise|AML.T0078]] | Drive-by Compromise | demonstrated |
| [[prompt-infiltration-via-public-facing-application|AML.T0093]] | Prompt Infiltration via Public-Facing Application | demonstrated |


## Case Studies (32)


The following case studies demonstrate this tactic:

- [[virustotal-poisoning|AML.CS0002: VirusTotal Poisoning]]
- [[camera-hijack-attack-on-facial-recognition-system|AML.CS0004: Camera Hijack Attack on Facial Recognition System]]
- [[tay-poisoning|AML.CS0009: Tay Poisoning]]
- [[microsoft-azure-service-disruption|AML.CS0010: Microsoft Azure Service Disruption]]
- [[face-identification-system-evasion-via-physical-countermeasures|AML.CS0012: Face Identification System Evasion via Physical Countermeasures]]
- [[backdoor-attack-on-deep-learning-models-in-mobile-apps|AML.CS0013: Backdoor Attack on Deep Learning Models in Mobile Apps]]
- [[compromised-pytorch-dependency-chain|AML.CS0015: Compromised PyTorch Dependency Chain]]
- [[achieving-code-execution-in-mathgpt-via-prompt-injection|AML.CS0016: Achieving Code Execution in MathGPT via Prompt Injection]]
- [[bypassing-id-me-identity-verification|AML.CS0017: Bypassing ID.me Identity Verification]]
- [[arbitrary-code-execution-with-google-colab|AML.CS0018: Arbitrary Code Execution with Google Colab]]
- [[poisongpt|AML.CS0019: PoisonGPT]]
- [[indirect-prompt-injection-threats-bing-chat-data-pirate|AML.CS0020: Indirect Prompt Injection Threats: Bing Chat Data Pirate]]
- [[chatgpt-conversation-exfiltration|AML.CS0021: ChatGPT Conversation Exfiltration]]
- [[chatgpt-package-hallucination|AML.CS0022: ChatGPT Package Hallucination]]
- [[shadowray|AML.CS0023: ShadowRay]]
- [[financial-transaction-hijacking-with-m365-copilot-as-an-insider|AML.CS0026: Financial Transaction Hijacking with M365 Copilot as an Insider]]
- [[organization-confusion-on-hugging-face|AML.CS0027: Organization Confusion on Hugging Face]]
- [[ai-model-tampering-via-supply-chain-attack|AML.CS0028: AI Model Tampering via Supply Chain Attack]]
- [[google-bard-conversation-exfiltration|AML.CS0029: Google Bard Conversation Exfiltration]]
- [[llm-jacking|AML.CS0030: LLM Jacking]]
- [[malicious-models-on-hugging-face|AML.CS0031: Malicious Models on Hugging Face]]
- [[attempted-evasion-of-ml-phishing-webpage-detection-system|AML.CS0032: Attempted Evasion of ML Phishing Webpage Detection System]]
- [[live-deepfake-image-injection-to-evade-mobile-kyc-verification|AML.CS0033: Live Deepfake Image Injection to Evade Mobile KYC Verification]]
- [[prokyc-deepfake-tool-for-account-fraud-attacks|AML.CS0034: ProKYC: Deepfake Tool for Account Fraud Attacks]]
- [[data-exfiltration-from-slack-ai-via-indirect-prompt-injection|AML.CS0035: Data Exfiltration from Slack AI via Indirect Prompt Injection]]
- [[aikatz-attacking-llm-desktop-applications|AML.CS0036: AIKatz: Attacking LLM Desktop Applications]]
- [[data-exfiltration-via-agent-tools-in-copilot-studio|AML.CS0037: Data Exfiltration via Agent Tools in Copilot Studio]]
- [[planting-instructions-for-delayed-automatic-ai-agent-tool-invocation|AML.CS0038: Planting Instructions for Delayed Automatic AI Agent Tool Invocation]]
- [[living-off-ai-prompt-injection-via-jira-service-management|AML.CS0039: Living Off AI: Prompt Injection via Jira Service Management]]
- [[hacking-chatgpt-s-memories-with-prompt-injection|AML.CS0040: Hacking ChatGPTâ€™s Memories with Prompt Injection]]
- [[rules-file-backdoor-supply-chain-attack-on-ai-coding-assistants|AML.CS0041: Rules File Backdoor: Supply Chain Attack on AI Coding Assistants]]
- [[lamehug-malware-leveraging-dynamic-ai-generated-commands|AML.CS0044: LAMEHUG: Malware Leveraging Dynamic AI-Generated Commands]]


## References

MITRE Corporation. *Initial Access (AML.TA0004)*. MITRE ATLAS. Available at: https://atlas.mitre.org/tactics/AML.TA0004
