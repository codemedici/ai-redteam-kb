---
id: extract-llm-system-prompt
title: "AML.T0056: Extract LLM System Prompt"
sidebar_label: "Extract LLM System Prompt"
sidebar_position: 6
---

# AML.T0056: Extract LLM System Prompt



Adversaries may attempt to extract a large language model's (LLM) system prompt. This can be done via prompt injection to induce the model to reveal its own system prompt or may be extracted from a configuration file.

System prompts can be a portion of an AI provider's competitive advantage and are thus valuable intellectual property that may be targeted by adversaries.

## Metadata

- **Technique ID:** AML.T0056
- **Created:** October 25, 2023
- **Last Modified:** March 12, 2025
- **Maturity:** feasible



## Tactics (1)

This technique supports the following tactics:


- [[exfiltration|AML.TA0010: Exfiltration]]




## Case Studies (0)

*No case studies currently documented for this technique.*

## References

MITRE Corporation. *Extract LLM System Prompt (AML.T0056)*. MITRE ATLAS. Available at: https://atlas.mitre.org/techniques/AML.T0056
