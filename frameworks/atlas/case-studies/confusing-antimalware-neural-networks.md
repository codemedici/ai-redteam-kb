---
id: confusing-antimalware-neural-networks
title: "AML.CS0014: Confusing Antimalware Neural Networks"
type: case-study
sidebar_position: 15
---

# AML.CS0014: Confusing Antimalware Neural Networks

Cloud storage and computations have become popular platforms for deploying ML malware detectors.
In such cases, the features for models are built on users' systems and then sent to cybersecurity company servers.
The Kaspersky ML research team explored this gray-box scenario and showed that feature knowledge is enough for an adversarial attack on ML models.

They attacked one of Kaspersky's antimalware ML models without white-box access to it and successfully evaded detection for most of the adversarially modified malware files.

## Metadata

- **ID:** AML.CS0014
- **Incident Date:** 2021-06-23
- **Type:** exercise
- **Target:** Kaspersky's Antimalware ML Model
- **Actor:** Kaspersky ML Research Team

## Procedure

### Step 1: Search Open AI Vulnerability Analysis

**Technique:** [[frameworks/atlas/techniques/reconnaissance/search-open-ai-vulnerability-analysis|AML.T0001: Search Open AI Vulnerability Analysis]]

The researchers performed a review of adversarial ML attacks on antimalware products.
They discovered that techniques borrowed from attacks on image classifiers have been successfully applied to the antimalware domain.
However, it was not clear if these approaches were effective against the ML component of production antimalware solutions.

### Step 2: Search Victim-Owned Websites

**Technique:** [[frameworks/atlas/techniques/reconnaissance/search-victim-owned-websites|AML.T0003: Search Victim-Owned Websites]]

Kaspersky's use of ML-based antimalware detectors is publicly documented on their website. In practice, an adversary could use this for targeting.

### Step 3: AI-Enabled Product or Service

**Technique:** [[frameworks/atlas/techniques/ai-model-access/ai-enabled-product-or-service|AML.T0047: AI-Enabled Product or Service]]

The researchers used access to the target ML-based antimalware product throughout this case study.
This product scans files on the user's system, extracts features locally, then sends them to the cloud-based ML malware detector for classification.
Therefore, the researchers had only black-box access to the malware detector itself, but could learn valuable information for constructing the attack from the feature extractor.

### Step 4: Datasets

**Technique:** [[frameworks/atlas/techniques/resource-development/acquire-public-ai-artifacts/acquire-public-AI-artifacts-datasets|AML.T0002.000: Datasets]]

The researchers collected a dataset of malware and clean files.
They scanned the dataset with the target ML-based antimalware solution and labeled the samples according to the ML detector's predictions.

### Step 5: Create Proxy AI Model

**Technique:** [[frameworks/atlas/techniques/ai-attack-staging/create-proxy-ai-model|AML.T0005: Create Proxy AI Model]]

A proxy model was trained on the labeled dataset of malware and clean files.
The researchers experimented with a variety of model architectures.

### Step 6: Adversarial AI Attacks

**Technique:** [[frameworks/atlas/techniques/resource-development/develop-capabilities/develop-capabilities-adversarial-AI-attacks|AML.T0017.000: Adversarial AI Attacks]]

By reverse engineering the local feature extractor, the researchers could collect information about the input features, used for the cloud-based ML detector.
The model collects PE Header features, section features and section data statistics, and file strings information.
A gradient based adversarial algorithm for executable files was developed.
The algorithm manipulates file features to avoid detection by the proxy model, while still containing the same malware payload

### Step 7: Black-Box Transfer

**Technique:** [[frameworks/atlas/techniques/ai-attack-staging/craft-adversarial-data/craft-adversarial-data-black-box-transfer|AML.T0043.002: Black-Box Transfer]]

Using a developed gradient-driven algorithm, malicious adversarial files for the proxy model were constructed from the malware files for black-box transfer to the target model.

### Step 8: Verify Attack

**Technique:** [[frameworks/atlas/techniques/ai-attack-staging/verify-attack|AML.T0042: Verify Attack]]

The adversarial malware files were tested against the target antimalware solution to verify their efficacy.

### Step 9: Evade AI Model

**Technique:** [[frameworks/atlas/techniques/initial-access/evade-ai-model|AML.T0015: Evade AI Model]]

The researchers demonstrated that for most of the adversarial files, the antimalware model was successfully evaded.
In practice, an adversary could deploy their adversarially crafted malware and infect systems while evading detection.

## References

1. [Article, "How to confuse antimalware neural networks. Adversarial attacks and protection"](https://securelist.com/how-to-confuse-antimalware-neural-networks-adversarial-attacks-and-protection/102949/)
