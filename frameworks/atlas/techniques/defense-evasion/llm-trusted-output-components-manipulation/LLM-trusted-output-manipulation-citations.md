---
id: LLM-trusted-output-manipulation-citations
title: "AML.T0067.000: Citations"
sidebar_label: "Citations"
sidebar_position: 67001
---

# AML.T0067.000: Citations

Adversaries may manipulate the citations provided in an AI system's response, in order to make it appear trustworthy. Variants include citing a providing the wrong citation, making up a new citation, or providing the right citation but for adversary-provided data.

## Metadata

- **Technique ID:** AML.T0067.000
- **Created:** 2025-03-12
- **Last Modified:** 2025-03-12
- **Maturity:** demonstrated

## Parent Technique

**Parent Technique:** AML.T0067 â€” LLM Trusted Output Components Manipulation

## Tactics (1)

- [[frameworks/atlas/tactics/defense-evasion|Defense Evasion]]

## Case Studies (1)

- [[frameworks/atlas/case-studies/financial-transaction-hijacking-with-m365-copilot-as-an-insider|Financial Transaction Hijacking with M365 Copilot as an Insider]]
