---
id: extract-llm-system-prompt
title: "AML.T0056: Extract LLM System Prompt"
sidebar_label: "Extract LLM System Prompt"
sidebar_position: 57
---

# AML.T0056: Extract LLM System Prompt

Adversaries may attempt to extract a large language model's (LLM) system prompt. This can be done via prompt injection to induce the model to reveal its own system prompt or may be extracted from a configuration file.

System prompts can be a portion of an AI provider's competitive advantage and are thus valuable intellectual property that may be targeted by adversaries.

## Metadata

- **Technique ID:** AML.T0056
- **Created:** 2023-10-25
- **Last Modified:** 2025-03-12
- **Maturity:** feasible

## Tactics (1)

- [[frameworks/atlas/tactics/exfiltration|Exfiltration]]
