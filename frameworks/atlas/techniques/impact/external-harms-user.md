---
id: external-harms-user-harm
title: "AML.T0048.003: User Harm"
sidebar_label: "User Harm"
sidebar_position: 9
---

# AML.T0048.003: User Harm

> **Sub-Technique of:** [[frameworks/atlas/techniques/impact/external-harms|AML.T0048: External Harms]]

User harms may encompass a variety of harm types including financial and reputational that are directed at or felt by individual victims of the attack rather than at the organization level.

## Metadata

- **Technique ID:** AML.T0048.003
- **Created:** October 25, 2023
- **Last Modified:** October 25, 2023
- **Maturity:** realized

- **Parent Technique:** [[frameworks/atlas/techniques/impact/external-harms|AML.T0048: External Harms]]

## Tactics (0)

This technique supports the following tactics:

*No tactics currently associated with this technique.*

## Case Studies (9)

The following case studies demonstrate this technique:

### [[frameworks/atlas/case-studies/indirect-prompt-injection-threats-bing-chat-data-pirate|AML.CS0020: Indirect Prompt Injection Threats: Bing Chat Data Pirate]]

With this user information, the attacker could now use the user's PII it has received for further identity-level attacks, such identity theft or fraud.

### [[frameworks/atlas/case-studies/chatgpt-conversation-exfiltration|AML.CS0021: ChatGPT Conversation Exfiltration]]

The user's privacy is violated, and they are potentially open to further targeted attacks.

### [[frameworks/atlas/case-studies/chatgpt-package-hallucination|AML.CS0022: ChatGPT Package Hallucination]]

This could lead to a variety of harms to the end user or organization.

### [[frameworks/atlas/case-studies/morris-ii-worm-rag-based-attack|AML.CS0024: Morris II Worm: RAG-Based Attack]]

Users of the GenAI email assistant may have PII leaked to attackers.

### [[frameworks/atlas/case-studies/google-bard-conversation-exfiltration|AML.CS0029: Google Bard Conversation Exfiltration]]

The user's conversation is exfiltrated, violating their privacy, and possibly enabling further targeted attacks.

### [[frameworks/atlas/case-studies/attempted-evasion-of-ml-phishing-webpage-detection-system|AML.CS0032: Attempted Evasion of ML Phishing Webpage Detection System]]

The end user may experience a variety of harms including financial and privacy harms depending on the credentials stolen by the adversary.

### [[frameworks/atlas/case-studies/aikatz-attacking-llm-desktop-applications|AML.CS0036: AIKatz: Attacking LLM Desktop Applications]]

The attacker could gain access to all of the victim’s activity with the LLM, including previous and ongoing chats, as well as any file or content uploaded to them.

### [[frameworks/atlas/case-studies/hacking-chatgpt-s-memories-with-prompt-injection|AML.CS0040: Hacking ChatGPT’s Memories with Prompt Injection]]

The victim can be misinformed, misled, or influenced as directed by ChatGPT's poisoned memories.

### [[frameworks/atlas/case-studies/rules-file-backdoor-supply-chain-attack-on-ai-coding-assistants|AML.CS0041: Rules File Backdoor: Supply Chain Attack on AI Coding Assistants]]

The victim developers unknowingly used the compromised AI coding assistant that generate code containing hidden malicious elements which could include backdoors, data exfiltration code, vulnerable constructs, or malicious scripts. This code could end up in a production application, affecting the users of the software.

## References

MITRE Corporation. *User Harm (AML.T0048.003)*. MITRE ATLAS. Available at: https://atlas.mitre.org/techniques/AML.T0048.003
