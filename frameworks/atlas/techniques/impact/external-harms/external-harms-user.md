---
id: external-harms-user
title: "AML.T0048.003: User Harm"
sidebar_label: "User Harm"
sidebar_position: 48004
---

# AML.T0048.003: User Harm

User harms may encompass a variety of harm types including financial and reputational that are directed at or felt by individual victims of the attack rather than at the organization level.

## Metadata

- **Technique ID:** AML.T0048.003
- **Created:** 2023-10-25
- **Last Modified:** 2023-10-25
- **Maturity:** realized

## Parent Technique

**Parent Technique:** AML.T0048 — External Harms

## Tactics (1)

- [[frameworks/atlas/tactics/impact|Impact]]

## Case Studies (10)

- [[frameworks/atlas/case-studies/indirect-prompt-injection-threats-bing-chat-data-pirate|Indirect Prompt Injection Threats: Bing Chat Data Pirate]]
- [[frameworks/atlas/case-studies/chatgpt-conversation-exfiltration|ChatGPT Conversation Exfiltration]]
- [[frameworks/atlas/case-studies/chatgpt-package-hallucination|ChatGPT Package Hallucination]]
- [[frameworks/atlas/case-studies/morris-ii-worm-rag-based-attack|Morris II Worm: RAG-Based Attack]]
- [[frameworks/atlas/case-studies/google-bard-conversation-exfiltration|Google Bard Conversation Exfiltration]]
- [[frameworks/atlas/case-studies/attempted-evasion-of-ml-phishing-webpage-detection-system|Attempted Evasion of ML Phishing Webpage Detection System]]
- [[frameworks/atlas/case-studies/aikatz-attacking-llm-desktop-applications|AIKatz: Attacking LLM Desktop Applications]]
- [[frameworks/atlas/case-studies/hacking-chatgpt-s-memories-with-prompt-injection|Hacking ChatGPT’s Memories with Prompt Injection]]
- [[frameworks/atlas/case-studies/rules-file-backdoor-supply-chain-attack-on-ai-coding-assistants|Rules File Backdoor: Supply Chain Attack on AI Coding Assistants]]
- [[frameworks/atlas/case-studies/exposed-clawdbot-control-interfaces-leads-to-credential-access-and-execution|Exposed ClawdBot Control Interfaces Leads to Credential Access and Execution]]
