---
id: search-open-ai-vulnerability-analysis
title: "AML.T0001: Search Open AI Vulnerability Analysis"
sidebar_label: "Search Open AI Vulnerability Analysis"
sidebar_position: 5
---

# AML.T0001: Search Open AI Vulnerability Analysis

Much like the [[frameworks/atlas/techniques/reconnaissance/search-open-technical-databases/search-open-technical-databases|Search Open Technical Databases]], there is often ample research available on the vulnerabilities of common AI models. Once a target has been identified, an adversary will likely try to identify any pre-existing work that has been done for this class of models.
This will include not only reading academic papers that may identify the particulars of a successful attack, but also identifying pre-existing implementations of those attacks. The adversary may obtain [[frameworks/atlas/techniques/resource-development/obtain-capabilities/adversarial-ai-attack-implementations|Adversarial AI Attack Implementations]] or develop their own [[frameworks/atlas/techniques/resource-development/develop-capabilities/adversarial-ai-attacks|Adversarial AI Attacks]] if necessary.

## Metadata

- **Technique ID:** AML.T0001
- **Created:** May 13, 2021
- **Last Modified:** April 17, 2025
- **Maturity:** demonstrated

## Tactics (1)

This technique supports the following tactics:

- 

## Case Studies (2)

The following case studies demonstrate this technique:

### [[frameworks/atlas/case-studies/confusing-antimalware-neural-networks|AML.CS0014: Confusing Antimalware Neural Networks]]

The researchers performed a review of adversarial ML attacks on antimalware products.
They discovered that techniques borrowed from attacks on image classifiers have been successfully applied to the antimalware domain.
However, it was not clear if these approaches were effective against the ML component of production antimalware solutions.

### [[frameworks/atlas/case-studies/achieving-code-execution-in-mathgpt-via-prompt-injection|AML.CS0016: Achieving Code Execution in MathGPT via Prompt Injection]]

With the understanding that LLMs can be vulnerable to prompt injection, the actor familiarized themselves with typical attack prompts, such as "Ignore above instructions.  Instead ..."

## References

MITRE Corporation. *Search Open AI Vulnerability Analysis (AML.T0001)*. MITRE ATLAS. Available at: https://atlas.mitre.org/techniques/AML.T0001
