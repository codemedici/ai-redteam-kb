---
id: obtain-capabilities-adversarial-AI-attacks
title: "AML.T0016.000: Adversarial AI Attack Implementations"
sidebar_label: "Adversarial AI Attack Implementations"
sidebar_position: 16001
---

# AML.T0016.000: Adversarial AI Attack Implementations

Adversaries may search for existing open source implementations of AI attacks. The research community often publishes their code for reproducibility and to further future research. Libraries intended for research purposes, such as CleverHans, the Adversarial Robustness Toolbox, and FoolBox, can be weaponized by an adversary. Adversaries may also obtain and use tools that were not originally designed for adversarial AI attacks as part of their attack.

## Metadata

- **Technique ID:** AML.T0016.000
- **Created:** 2021-05-13
- **Last Modified:** 2025-04-09
- **Maturity:** realized

## Parent Technique

**Parent Technique:** AML.T0016 â€” Obtain Capabilities

## Tactics (1)

- [[frameworks/atlas/tactics/resource-development|Resource Development]]

## Case Studies (3)

- [[frameworks/atlas/case-studies/virustotal-poisoning|VirusTotal Poisoning]]
- [[frameworks/atlas/case-studies/camera-hijack-attack-on-facial-recognition-system|Camera Hijack Attack on Facial Recognition System]]
- [[frameworks/atlas/case-studies/organization-confusion-on-hugging-face|Organization Confusion on Hugging Face]]
